{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reserved-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m25dehgh/testing_codes/NNs\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3 -u\n",
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the LICENSE file in\n",
    "# the root directory of this source tree.\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import models\n",
    "from utils import progress_bar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "# parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                     help='resume from checkpoint')\n",
    "# parser.add_argument('--model', default=\"ResNet18\", type=str,\n",
    "#                     help='model type (default: ResNet18)')\n",
    "# parser.add_argument('--name', default='0', type=str, help='name of run')\n",
    "# parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "# parser.add_argument('--batch-size', default=128, type=int, help='batch size')\n",
    "# parser.add_argument('--epoch', default=200, type=int,\n",
    "#                     help='total epochs to run')\n",
    "# parser.add_argument('--no-augment', dest='augment', action='store_false',\n",
    "#                     help='use standard augmentation (default: True)')\n",
    "# parser.add_argument('--decay', default=1e-4, type=float, help='weight decay')\n",
    "# parser.add_argument('--alpha', default=1., type=float,\n",
    "#                     help='mixup interpolation coefficient (default: 1)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "params = {\n",
    "    \"lr\": .01,\n",
    "    \"resume\": False,\n",
    "    \"model\": \"ResNet18\",\n",
    "    \"name\": \"mixup-128-NormalAdvsTrain\",\n",
    "    \"seed\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"decay\": 5e-4, \n",
    "    \"augment\": True,\n",
    "    \"epoch\": 200,\n",
    "#     \"no_augment\": False,\n",
    "    \"alpha\": 0.,\n",
    "}\n",
    "\n",
    "# %cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-merchant",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "if params[\"seed\"] != 0:\n",
    "    torch.manual_seed(params[\"seed\"])\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "if params[\"augment\"]:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "else:\n",
    "    print(\"no augmentation\")\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=False,\n",
    "                            transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=params[\"batch_size\"],\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=False,\n",
    "                           transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-heaven",
   "metadata": {},
   "source": [
    "# Creating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n",
      "3\n",
      "Using CUDA..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "if params[\"resume\"]:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.t7' + params[\"name\"] + '_'\n",
    "                            + str(params[\"seed\"]), map_location='cpu')\n",
    "    \n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    rng_state = checkpoint['rng_state']\n",
    "    torch.set_rng_state(rng_state)\n",
    "else:\n",
    "    print('==> Building model..')\n",
    "    net = models.__dict__[params[\"model\"]]()\n",
    "\n",
    "if not os.path.isdir('results'):\n",
    "    os.mkdir('results')\n",
    "logname = ('results/log_' + net.__class__.__name__ + '_' + params[\"name\"] + '_'\n",
    "           + str(params[\"seed\"]) + '.csv')\n",
    "\n",
    "if use_cuda:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    net.to(f'cuda:{net.device_ids[0]}')\n",
    "    print(torch.cuda.device_count())\n",
    "    cudnn.benchmark = True\n",
    "    print('Using CUDA..')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=params[\"lr\"], momentum=0.9,\n",
    "                      weight_decay=params[\"decay\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-mason",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innocent-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True, advs_train=False, epsilon=.35):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    \n",
    "\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if advs_train:\n",
    "        \n",
    "        x_b = x[index, :]\n",
    "        \n",
    "        x_b.requires_grad_(True)\n",
    "        x.requires_grad_(True)\n",
    "        \n",
    "        out_a = net(x)\n",
    "        loss_a = criterion(out_a, y_a)\n",
    "        optimizer.zero_grad()\n",
    "        loss_a.backward()\n",
    "        \n",
    "        err = torch.zeros_like(x)\n",
    "        with torch.no_grad():\n",
    "            err += epsilon * lam * x.grad.sign()\n",
    "\n",
    "        out_b = net(x_b)\n",
    "        loss_b = criterion(out_b, y_b)\n",
    "        optimizer.zero_grad()\n",
    "        loss_b.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            err += epsilon * (1 - lam) * x_b.grad.sign()\n",
    "            \n",
    "        mixed_x += err\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_mixup(epoch, erm=False, advs_train=False, epsilon=.35):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    reg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        if erm:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets,\n",
    "                                                           alpha=0, use_cuda=use_cuda, advs_train=advs_train)            \n",
    "        else:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets,\n",
    "                                                           params[\"alpha\"], use_cuda, \n",
    "                                                           advs_train=advs_train, epsilon=epsilon)\n",
    "        inputs, targets_a, targets_b = map(Variable, (inputs,\n",
    "                                                      targets_a, targets_b))\n",
    "        outputs = net(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader),\n",
    "                     'Loss: %.3f | Reg: %.5f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), reg_loss/(batch_idx+1),\n",
    "                        100.*correct/total, correct, total))\n",
    "    return (train_loss/batch_idx, reg_loss/batch_idx, 100.*correct/total)\n",
    "\n",
    "\n",
    "def train_normal(epochs, advs_train=False , epsilon=.35):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    reg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs_normal = net(inputs)\n",
    "        loss_normal = criterion(outputs_normal, targets)\n",
    "        \n",
    "        if advs_train:\n",
    "            inputs_advs = fgsm(inputs, targets, eps=epsilon)\n",
    "            outputs_advs = net(inputs_advs)\n",
    "            loss_advs = criterion(outputs_advs, targets)\n",
    "            loss = loss_advs + loss_normal\n",
    "        else:\n",
    "            loss = loss_normal\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs_normal.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().float()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader),\n",
    "                     'Loss: %.3f | Reg: %.5f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), reg_loss/(batch_idx+1),\n",
    "                        100.*correct/total, correct, total))\n",
    "        \n",
    "    return (train_loss/batch_idx, reg_loss/batch_idx, 100.*correct/total)\n",
    "\n",
    "\n",
    "def fgsm(x, y, eps=.35):\n",
    "    \n",
    "    x.requires_grad_(True)\n",
    "    out = net(x)\n",
    "    loss = criterion(out, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        err = eps * x.grad.sign()    \n",
    "        x_advs = x + err\n",
    "\n",
    "    return x_advs\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        progress_bar(batch_idx, len(testloader),\n",
    "                     'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (test_loss/(batch_idx+1), 100.*correct/total,\n",
    "                        correct, total))\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        checkpoint(acc, epoch)\n",
    "\n",
    "        \n",
    "    return (test_loss/batch_idx, 100.*correct/total)\n",
    "\n",
    "\n",
    "def checkpoint(acc, epoch):\n",
    "    # Save checkpoint.\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net,\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/ckpt.t7' + params[\"name\"] + '_'\n",
    "               + str(params[\"seed\"]))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
    "    lr = params[\"lr\"]\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-oxford",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-navigation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [=====================================================================================>]  Step: 348ms | Tot: 58s43ms | Loss: 4.015 | Reg: 0.00000 | Acc: 32.358% (16179/50000 391/391  \n",
      " [=>....................................................................................]  Step: 35ms | Tot: 84ms | Loss: 1.800 | Acc: 34.667% (104/300 3/100 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f2d5d243f5c6>:159: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=====================================================================================>]  Step: 36ms | Tot: 3s620ms | Loss: 1.826 | Acc: 33.810% (3381/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [=====================================================================================>]  Step: 141ms | Tot: 57s940ms | Loss: 3.139 | Reg: 0.00000 | Acc: 49.858% (24929/50000 391/391 \n",
      " [=====================================================================================>]  Step: 37ms | Tot: 3s637ms | Loss: 1.494 | Acc: 45.670% (4567/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [=====================================================================================>]  Step: 148ms | Tot: 58s66ms | Loss: 2.439 | Reg: 0.00000 | Acc: 61.648% (30824/50000 391/391  \n",
      " [=====================================================================================>]  Step: 43ms | Tot: 3s631ms | Loss: 1.509 | Acc: 48.410% (4841/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      " [=====================================================================================>]  Step: 136ms | Tot: 58s104ms | Loss: 1.735 | Reg: 0.00000 | Acc: 69.092% (34546/50000 391/391 \n",
      " [=====================================================================================>]  Step: 35ms | Tot: 3s648ms | Loss: 1.094 | Acc: 63.950% (6395/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      " [=====================================================================================>]  Step: 139ms | Tot: 58s200ms | Loss: 1.114 | Reg: 0.00000 | Acc: 74.512% (37256/50000 391/391 \n",
      " [=====================================================================================>]  Step: 42ms | Tot: 3s662ms | Loss: 0.876 | Acc: 70.990% (7099/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 5\n",
      " [=====================================================================================>]  Step: 141ms | Tot: 58s79ms | Loss: 0.858 | Reg: 0.00000 | Acc: 78.846% (39423/50000 391/391  \n",
      " [=====================================================================================>]  Step: 37ms | Tot: 3s678ms | Loss: 0.711 | Acc: 76.200% (7620/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 6\n",
      " [=====================================================================================>]  Step: 140ms | Tot: 58s150ms | Loss: 0.738 | Reg: 0.00000 | Acc: 81.114% (40557/50000 391/391 \n",
      " [=====================================================================================>]  Step: 38ms | Tot: 3s647ms | Loss: 0.731 | Acc: 75.480% (7548/10000 100/100 \n",
      "\n",
      "Epoch: 7\n",
      " [=====================================================================================>]  Step: 146ms | Tot: 58s594ms | Loss: 0.650 | Reg: 0.00000 | Acc: 82.892% (41446/50000 391/391 \n",
      " [=====================================================================================>]  Step: 35ms | Tot: 3s680ms | Loss: 0.652 | Acc: 78.660% (7866/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 8\n",
      " [=====================================================================================>]  Step: 130ms | Tot: 58s485ms | Loss: 0.572 | Reg: 0.00000 | Acc: 84.696% (42348/50000 391/391 \n",
      " [=====================================================================================>]  Step: 41ms | Tot: 3s683ms | Loss: 0.620 | Acc: 80.070% (8007/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 9\n",
      " [=====================================================================================>]  Step: 149ms | Tot: 58s556ms | Loss: 0.534 | Reg: 0.00000 | Acc: 85.748% (42874/50000 391/391 \n",
      " [=====================================================================================>]  Step: 37ms | Tot: 3s666ms | Loss: 0.591 | Acc: 80.850% (8085/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 10\n",
      " [=====================================================================================>]  Step: 134ms | Tot: 58s715ms | Loss: 0.480 | Reg: 0.00000 | Acc: 86.750% (43375/50000 391/391 \n",
      " [=====================================================================================>]  Step: 35ms | Tot: 3s614ms | Loss: 0.559 | Acc: 81.980% (8198/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 11\n",
      " [=====================================================================================>]  Step: 138ms | Tot: 58s701ms | Loss: 0.445 | Reg: 0.00000 | Acc: 87.694% (43847/50000 391/391 \n",
      " [=====================================================================================>]  Step: 37ms | Tot: 3s599ms | Loss: 0.528 | Acc: 83.300% (8330/10000 100/100 : 37ms | Tot: 223ms | Loss: 0.516 | Acc: 83.250% (666/800 8/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 12\n",
      " [=====================================================================================>]  Step: 145ms | Tot: 58s856ms | Loss: 0.418 | Reg: 0.00000 | Acc: 88.504% (44252/50000 391/391 \n",
      " [=====================================================================================>]  Step: 36ms | Tot: 3s661ms | Loss: 0.508 | Acc: 83.290% (8329/10000 100/100 \n",
      "\n",
      "Epoch: 13\n",
      " [=====================================================================================>]  Step: 157ms | Tot: 58s825ms | Loss: 0.393 | Reg: 0.00000 | Acc: 89.082% (44541/50000 391/391 \n",
      " [=====================================================================================>]  Step: 32ms | Tot: 3s671ms | Loss: 0.510 | Acc: 82.940% (8294/10000 100/100 \n",
      "\n",
      "Epoch: 14\n",
      " [=====================================================================================>]  Step: 140ms | Tot: 58s667ms | Loss: 0.371 | Reg: 0.00000 | Acc: 89.604% (44802/50000 391/391 \n",
      " [=====================================================================================>]  Step: 38ms | Tot: 3s653ms | Loss: 0.483 | Acc: 84.600% (8460/10000 100/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 15\n",
      " [=====================================================================================>]  Step: 140ms | Tot: 58s675ms | Loss: 0.343 | Reg: 0.00000 | Acc: 90.306% (45153/50000 391/391 \n",
      " [=====================================================================================>]  Step: 35ms | Tot: 3s678ms | Loss: 0.520 | Acc: 83.870% (8387/10000 100/100 \n",
      "\n",
      "Epoch: 16\n",
      " [===================================================================>..................]  Step: 145ms | Tot: 46s11ms | Loss: 0.324 | Reg: 0.00000 | Acc: 90.793% (35678/39296 307/391  \r"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(logname):\n",
    "    with open(logname, 'w') as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=',')\n",
    "        logwriter.writerow(['epoch', 'train loss', 'reg loss', 'train acc',\n",
    "                            'test loss', 'test acc'])\n",
    "\n",
    "for epoch in range(start_epoch, params[\"epoch\"]):\n",
    "    train_loss, reg_loss, train_acc = train_normal(epoch, advs_train=True)\n",
    "    test_loss, test_acc = test(epoch)\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    with open(logname, 'a') as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=',')\n",
    "        logwriter.writerow([epoch, train_loss, reg_loss, train_acc, test_loss,\n",
    "                            test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-lucas",
   "metadata": {},
   "source": [
    "# Robustness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "attempted-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_robust(epsilon=0):\n",
    "#     net.cuda()\n",
    "#     net.to('cpu')\n",
    "    net.to(f'cuda:{net.device_ids[0]}')\n",
    "    net.eval()\n",
    "    incorrect = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (normal_inpts, targets) in tqdm(enumerate(testloader)):\n",
    "        if use_cuda:\n",
    "            normal_inpts, targets = normal_inpts.cuda(), targets.cuda()\n",
    "#             normal_inpts, targets = normal_inpts.to('cuda:1'), targets.to('cuda:1')\n",
    "#         normal_inpts, targets = Variable(normal_inpts, volatile=True), Variable(targets)\n",
    "        normal_inpts.requires_grad_(True)\n",
    "        \n",
    "        outputs = net(normal_inpts)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            advers_inp = normal_inpts + (epsilon * normal_inpts.grad.sign())\n",
    "            advers_out = net(advers_inp)\n",
    "#             print(advers_out.cpu().data.shape)\n",
    "            normal_out = net(normal_inpts)\n",
    "            _,advs_predicted = torch.max(advers_out.data, 1)\n",
    "            _,normal_predicted = torch.max(normal_out.data, 1)\n",
    "    \n",
    "\n",
    "#         _, advs_predicted = torch.max(advs_predicted.data, 1)\n",
    "        total += targets.size(0)\n",
    "        incorrect += targets.size(0) - advs_predicted.eq(targets.data).sum()\n",
    "    \n",
    "    print(f'incorrects are: {incorrect} \\ntotal is: {total} \\nTop-1 error is {100.*incorrect/total}')\n",
    "        \n",
    "\n",
    "#     acc = 100.*correct/total\n",
    "#     return (test_loss/batch_idx, 100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "regional-petroleum",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:08, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incorrects are: 446 \n",
      "total is: 10000 \n",
      "Top-1 error is 4.460000038146973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_robust(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "straight-december",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(84.3200)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "affiliated-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.imshow(npimg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "located-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(94.6600)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "comparative-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels is 1 \n",
      " adversaliral predicted is tensor([6]) \n",
      " nomral is predictedtensor([1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAO70lEQVR4nO3dfawc1XnH8e9TYqANKMG4gAV2AZeGIJIY6wqQoCkhLXIREkYqaYjUWkmaG1VBDVL4wwIpOJHS0ApIaRQRmZfEaQkvKRBQS1uQE0SdPxzMq03Ni4kIUFw7lBBASYsxT//YsXRxdmavd2dnLz7fj2Tt7jk7M49H/nl25+ycicxE0r7vNyZdgKRuGHapEIZdKoRhlwph2KVCGHapEO8aZeGIWA5cDewHXJeZlw94v+N82mdFQ9+yE3+vvvOAg/s2P7n1wdpF5h/Rv/1//htefyX7lhLDjrNHxH7AU8AfAS8ADwAXZOZ/Nixj2LXPOrCh71db19V3LvlI3+Y/OKf+g/cFq/q3f/Uv4KdP9A/7KB/jTwa2ZuZPMvMN4Gbg3BHWJ2mMRgn7kcDzM16/ULVJmoNG+c7e76PCr31Mj4hpYHqE7UhqwShhfwFYNOP1UcCLe74pM9cAa8Dv7NIkjfIx/gHguIg4JiL2Bz4O3NVOWZLaNvSRPTPfjIgLgX+nN/R2Q2Y+3lpl0jvM/zb0xe9+dK/Xd9lf1fc9+cOaGl6rX2akcfbMvBu4e5R1SOqGv6CTCmHYpUIYdqkQhl0qhGGXCjHS2XhJ47N+Q33f8cv6t+fO+mU8skuFMOxSIQy7VAjDLhXCsEuFGHpaqqE25iWu0thltj8tlaR3EMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUiJHmoIuIZ4HXgF3Am5k51UZRktrXxoSTH8nMl1pYj6Qx8mO8VIhRw57APRHxYERMt1GQpPEY9WP8aZn5YkQcBtwbEU9k5v0z31D9J+B/BNKEtTYtVUSsBl7PzCsa3uO0VNKYtT4tVUS8OyIO3v0cOAvYPOz6JI3XKB/jDwfuiIjd6/luZv5bK1VJap2zy0r7GGeXlQpn2KVCGHapEIZdKoRhlwph2KVCGHapEIZdKoRhlwph2KVCGHapEIZdKoRhlwph2KVCGHapEIZdKoRhlwph2KVCGHapEIZdKoRhlwph2KVCGHapEIZdKoRhlwoxMOwRcUNE7IiIzTPa5kfEvRHxdPV4yHjLlDSq2RzZvw0s36NtFbAuM48D1lWvJc1hA8Ne3W/95T2azwXWVs/XAivaLUtS24b9zn54Zm4DqB4Pa68kSeMwyi2bZyUipoHpcW9HUrNhj+zbI2IhQPW4o+6NmbkmM6cyc2rIbUlqwbBhvwtYWT1fCdzZTjmSxiUys/kNETcBZwALgO3AZcD3gVuBxcBzwPmZuedJvH7rat6YpJFlZvRrHxj2Nhl2afzqwu4v6KRCGHapEIZdKoRhlwph2KVCjP0XdCrTJ2vav9VpFZrJI7tUCMMuFcKwS4Uw7FIhDLtUCMMuFcILYTS0tQ19f/7K3/VtP+K9F9Uus32karSbF8JIhTPsUiEMu1QIwy4VwrBLhfBsvMZiU037iafXL/Pw+vq+kz6xsLYvvrttdkUVwrPxUuEMu1QIwy4VwrBLhTDsUiEMu1SIgXPQRcQNwDnAjsw8sWpbDXwG+Fn1tksy8+5xFam56aGt9SOp6y+/sm/7idddXLvMPQ3bOqlheG1L/agc7x9iVO6Tpyyp7fvWhmf2foVzxGyO7N8Glvdp/1pmLq3+GHRpjhsY9sy8Hxh400ZJc9so39kvjIjHIuKGiDiktYokjcWwYb8GWAIsBbYB/b+gARExHREbI2LjkNuS1IKhwp6Z2zNzV2a+BVwLnNzw3jWZOZWZU8MWKWl0Q4U9Imae/zwP2NxOOZLGZeBVbxFxE3AGsIDeNGGXVa+XAgk8C3w2MwcOcnjV275lqCsm179R2xW/f0Bt34ENq/zVD75c2/fNM7/Yt/0vG9aXP7iltu9Tf39Vbd+3vr+hYa1776iGvkNr2p8Cfllz1dvAcfbMvKBP8/WDlpM0t/gLOqkQhl0qhGGXCmHYpUIYdqkQA8/Ga+5pGvB6X037U2OoI6LvCA/QMCz3va/XLvOhhm092tD3TM3wGsDzNe0XNazvXWf+aW3frobljqm/WI77XqrvW7zs/JqeX9Qv9IEP9G2euvkfaxfxyC4VwrBLhTDsUiEMu1QIwy4VwrBLhXDobYLGcQngkzXt9YNk43H3Oaf2bZ/3L/VXhj2yomGFdwy3t75S0940bNjk8IbhtUt3Lq7tW7ysvo/aq+yGqPFH99V2eWSXCmHYpUIYdqkQhl0qhGGXCjFwDrpWN1bgHHRz5S+8uqHvS10VQfO8anUXrfTMa+jbOVQt+6IpYGPNHHQe2aVCGHapEIZdKoRhlwph2KVCGHapELO5/dMi4DvAEcBbwJrMvDoi5gO3AEfTuwXUxzLz5wPWNVdGooayvKb9XzutQq1YvKK+b9kZDX3H1/ctaLjY5aXn6vt+q2ZY8RNnNizTv3nqjCk2Prxx6KG3N4EvZOb7gVOBz0XECcAqYF1mHgesq15LmqMGhj0zt2XmQ9Xz14AtwJHAucDa6m1rgRVjqlFSC/bqO3tEHA2cBGwADt9959bq8bDWq5PUmllPXhERBwG3ARdl5quzvfg/IqaB6eHKk9SWWR3ZI2IevaDfmJm3V83bI2Jh1b8Q2NFv2cxck5lTmTnVRsGShjMw7NE7hF8PbMnMmXeivwtYWT1fCdzZfnmS2jKbobfTgf8ANtEbegO4hN739luBxcBzwPmZ+fKAdXU29HZMQ98RDX0NN9zh8SFr6VbNMM6yr9YvsuJP6vuWNAwnPfFMfd91V/Vv33ZN/TKNGoa8aBjW4pdDbm+uW9C3dYpX2Jg7+37HHvidPTPXUz/z3UdnXZukifIXdFIhDLtUCMMuFcKwS4Uw7FIh9tkJJy9p6Du2oW9ZQ99JQ9bSqXln9W/feU+3dWginHBSkmGXSmHYpUIYdqkQhl0qhGGXCjHrySveabY19DVdI/XptgvpmkNsquGRXSqEYZcKYdilQhh2qRCGXSrEPns2/tyGvpc6q2I8mmZVq5mBrrZd5fDILhXCsEuFMOxSIQy7VAjDLhXCsEuFGDj0FhGLgO/Qu2vSW8CazLw6IlYDnwF+Vr31ksy8e1yF7q33Ndy16LSmK2E6NLv74E5WdzMUatxmM87+JvCFzHwoIg4GHoyIe6u+r2XmFeMrT1JbZnOvt21UV4xm5msRsQU4ctyFSWrXXn1nj4ij6c2ovKFqujAiHouIGyLikLaLk9SeWYc9Ig4CbgMuysxXgWuAJcBSekf+K2uWm46IjRGxcfRyJQ1rVmGPiHn0gn5jZt4OkJnbM3NXZr4FXAuc3G/ZzFyTmVOZOdVW0ZL23sCwR0QA1wNbMvOqGe0LZ7ztPGBz++VJastszsafBvwZsCkiHqnaLgEuiIil9EZnngU+O4b6hvZ8w/Da8d2VwY863BbAfjXtuzqtQnPRbM7Gr6f/kPCcGVOXNJi/oJMKYdilQhh2qRCGXSqEYZcKsc9OOPmehqve7mwYlmuaqHIYp7e8vkHaHmJrujLPK+Im5+s17TsalvHILhXCsEuFMOxSIQy7VAjDLhXCsEuF2GeH3hY1DL1d1TD01nTp3vahq2nXdENf3T3dvjGOQjQxz9e0v9GwjEd2qRCGXSqEYZcKYdilQhh2qRCGXSrEPjv0tvCU99T2LVn/i9q+W8ZRTMsu/eena/t+vPb6vu3f+N7lrddxaEPfy61vTaPyyC4VwrBLhTDsUiEMu1QIwy4VYuDZ+Ig4ELgfOKB6/z9l5mURMZ/eyeuj6d3+6WOZ+fPxlbqXrvjr2q6vLPxmbd+GizfV9q0bqaD2fOrzV9X2nX7KaZ3V4Rn3d5bZHNn/DzgzMz9E7/bMyyPiVGAVsC4zj6OXg1Vjq1LSyAaGPXter17Oq/4kvYlY11bta4EV4yhQUjtme3/2/ao7uO4A7s3MDcDhmbkNoHo8bGxVShrZrMKembsycylwFHByRJw42w1ExHREbIyIjUPWKKkFe3U2PjNfAe4DlgPbI2IhQPXYd376zFyTmVOZOTVaqZJGMTDsEfHbEfHe6vlvAn8IPAHcBays3rYSuHNMNUpqQWQ238QnIj5I7wTcfvT+c7g1M78cEYcCtwKLgeeA8zOzcTQmIrxjkDRmmdn3rl0Dw94mwy6NX13Y/QWdVAjDLhXCsEuFMOxSIQy7VIiu56B7Cfhp9XxB9XrSrOPtrOPt3ml1/E5dR6dDb2/bcMTGufCrOuuwjlLq8GO8VAjDLhVikmFfM8Ftz2Qdb2cdb7fP1DGx7+ySuuXHeKkQEwl7RCyPiCcjYmtETGzuuoh4NiI2RcQjXU6uERE3RMSOiNg8o21+RNwbEU9Xj4dMqI7VEfFf1T55JCLO7qCORRHxw4jYEhGPR8Tnq/ZO90lDHZ3uk4g4MCJ+HBGPVnV8qWofbX9kZqd/6F0q+wxwLLA/8ChwQtd1VLU8CyyYwHY/DCwDNs9o+1tgVfV8FfA3E6pjNXBxx/tjIbCsen4w8BRwQtf7pKGOTvcJEMBB1fN5wAbg1FH3xySO7CcDWzPzJ5n5BnAzvckri5GZ9/PrMzF3PoFnTR2dy8xtmflQ9fw1YAtwJB3vk4Y6OpU9rU/yOomwHwk8P+P1C0xgh1YSuCciHoyI6QnVsNtcmsDzwoh4rPqYP/avEzNFxNHASfSOZhPbJ3vUAR3vk3FM8jqJsPe7sH5SQwKnZeYy4I+Bz0XEhydUx1xyDbCE3j0CtgFXdrXhiDgIuA24KDNf7Wq7s6ij832SI0zyWmcSYX8BWDTj9VHAixOog8x8sXrcAdxB7yvGpMxqAs9xy8zt1T+0t4Br6WifRMQ8egG7MTNvr5o73yf96pjUPqm2/Qp7OclrnUmE/QHguIg4JiL2Bz5Ob/LKTkXEuyPi4N3PgbOAzc1LjdWcmMBz9z+mynl0sE8iIoDrgS2ZOfPeVp3uk7o6ut4nY5vktaszjHucbTyb3pnOZ4BLJ1TDsfRGAh4FHu+yDuAmeh8Hd9L7pPNp4FB6t9F6unqcP6E6/gHYBDxW/eNa2EEdp9P7KvcY8Ej15+yu90lDHZ3uE+CDwMPV9jYDX6zaR9of/oJOKoS/oJMKYdilQhh2qRCGXSqEYZcKYdilQhh2qRCGXSrE/wNVluA4SnKycAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = .11\n",
    "i = random.randint(0,100)\n",
    "# i = 1\n",
    "\n",
    "\n",
    "normal_inpt = trainset[i][0].reshape(1,3,32,32).cuda()\n",
    "normal_inpt.requires_grad_(True)\n",
    "out = net(normal_inpt)\n",
    "\n",
    "target = torch.tensor([trainset[i][1]]).cuda()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    advers_inp = normal_inpt + (epsilon * normal_inpt.grad.sign())\n",
    "    advers_out = net(advers_inp)\n",
    "    normal_out = net(normal_inpt)\n",
    "    _,advs_predicted = torch.max(advers_out.cpu().data, 1)\n",
    "    _,normal_predicted = torch.max(normal_out.cpu().data, 1)\n",
    "    \n",
    "print(f'true labels is {target.item()} \\n adversaliral predicted is {advs_predicted} \\n nomral is predicted{normal_predicted}')\n",
    "\n",
    "imshow(normal_inpt.cpu().detach().squeeze())\n",
    "imshow(advers_inp.cpu().detach().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "inclusive-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4291)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# advers_out = net(advers_inp) #+ (epsilon * normal_inpt.grad.sign())\n",
    "# print(advers_out)\n",
    "mx = 100\n",
    "for j in range(1000):\n",
    "    i = random.randint(0,3000)\n",
    "    if torch.min(torch.min(trainset[i][0], -1)[0], -1)[0][0] < mx:\n",
    "        mx = torch.min(torch.min(trainset[i][0], -1)[0], -1)[0][0]\n",
    "mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "sufficient-repository",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>reg loss</th>\n",
       "      <th>train acc</th>\n",
       "      <th>test loss</th>\n",
       "      <th>test acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.854828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(35.1079)</td>\n",
       "      <td>1.217799</td>\n",
       "      <td>tensor(57.0300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.580279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(48.6740)</td>\n",
       "      <td>0.930699</td>\n",
       "      <td>tensor(69.6800)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.464894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(54.0907)</td>\n",
       "      <td>0.964110</td>\n",
       "      <td>tensor(68.2700)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.409389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(56.5027)</td>\n",
       "      <td>0.768147</td>\n",
       "      <td>tensor(76.2700)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.358588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(58.5401)</td>\n",
       "      <td>0.707272</td>\n",
       "      <td>tensor(78.8900)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>0.770341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(74.8484)</td>\n",
       "      <td>0.297381</td>\n",
       "      <td>tensor(95.1700)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>0.745491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(75.9486)</td>\n",
       "      <td>0.265202</td>\n",
       "      <td>tensor(95.3000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>0.749481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(75.8501)</td>\n",
       "      <td>0.252477</td>\n",
       "      <td>tensor(95.4300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(75.3202)</td>\n",
       "      <td>0.269733</td>\n",
       "      <td>tensor(95.2800)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>0.747603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tensor(75.9354)</td>\n",
       "      <td>0.251721</td>\n",
       "      <td>tensor(95.3500)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  train loss  reg loss        train acc  test loss         test acc\n",
       "0        0    1.854828       0.0  tensor(35.1079)   1.217799  tensor(57.0300)\n",
       "1        1    1.580279       0.0  tensor(48.6740)   0.930699  tensor(69.6800)\n",
       "2        2    1.464894       0.0  tensor(54.0907)   0.964110  tensor(68.2700)\n",
       "3        3    1.409389       0.0  tensor(56.5027)   0.768147  tensor(76.2700)\n",
       "4        4    1.358588       0.0  tensor(58.5401)   0.707272  tensor(78.8900)\n",
       "..     ...         ...       ...              ...        ...              ...\n",
       "195    195    0.770341       0.0  tensor(74.8484)   0.297381  tensor(95.1700)\n",
       "196    196    0.745491       0.0  tensor(75.9486)   0.265202  tensor(95.3000)\n",
       "197    197    0.749481       0.0  tensor(75.8501)   0.252477  tensor(95.4300)\n",
       "198    198    0.763400       0.0  tensor(75.3202)   0.269733  tensor(95.2800)\n",
       "199    199    0.747603       0.0  tensor(75.9354)   0.251721  tensor(95.3500)\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"results/log_ResNet_0_0.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "tropical-punch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([torch.exp(torch.tensor(1.))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-computer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "hugging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
